{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0124b8d",
   "metadata": {},
   "source": [
    "### Tokenization ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b34aadf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus= \"\"\"To generate a paragraph on nature, you can use one of many AI-powered paragraph generator tools available online. \n",
    "These tools use artificial intelligence and natural language processing to create original content based on your specifications, such as topic, tone, and length.\n",
    "The transformer architecture is a deep learning model that uses the self-attention mechanism to process sequential data, like text, without relying on recurrence. \n",
    "This architecture, introduced in the 2017 paper \"Attention Is All You Need\", is structured around an encoder and a decoder, which efficiently process an input sequence and generate an output sequence by capturing relationships between elements regardless of their distance. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43cda781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To generate a paragraph on nature, you can use one of many AI-powered paragraph generator tools available online.', 'These tools use artificial intelligence and natural language processing to create original content based on your specifications, such as topic, tone, and length.']\n",
      "['To', 'generate', 'a', 'paragraph', 'on', 'nature', ',', 'you', 'can', 'use', 'one', 'of', 'many', 'AI-powered', 'paragraph', 'generator', 'tools', 'available', 'online', '.', 'These', 'tools', 'use', 'artificial', 'intelligence', 'and', 'natural', 'language', 'processing', 'to', 'create', 'original', 'content', 'based', 'on', 'your', 'specifications', ',', 'such', 'as', 'topic', ',', 'tone', ',', 'and', 'length', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "sentences = sent_tokenize(corpus)\n",
    "words = word_tokenize(corpus)\n",
    "print(sentences)\n",
    "print(words)\n",
    "type(sentences)\n",
    "type(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6286c4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To', 'generate', 'a', 'paragraph', 'on', 'nature', ',', 'you', 'can', 'use', 'one', 'of', 'many', 'AI', '-', 'powered', 'paragraph', 'generator', 'tools', 'available', 'online', '.', 'These', 'tools', 'use', 'artificial', 'intelligence', 'and', 'natural', 'language', 'processing', 'to', 'create', 'original', 'content', 'based', 'on', 'your', 'specifications', ',', 'such', 'as', 'topic', ',', 'tone', ',', 'and', 'length', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "word_punct = wordpunct_tokenize(corpus)\n",
    "print(word_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1aa6ed",
   "metadata": {},
   "source": [
    "### Stemming ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b56d787c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat --> eat\n",
      "eating --> eat\n",
      "eaten --> eaten\n",
      "eats --> eat\n",
      "ran --> ran\n",
      "running --> run\n",
      "runs --> run\n",
      "runner --> runner\n",
      "ate --> ate\n",
      "bats --> bat\n",
      "batted --> bat\n",
      "battering --> batter\n",
      "dance --> danc\n",
      "danced --> danc\n",
      "dancing --> danc\n"
     ]
    }
   ],
   "source": [
    "words=['eat','eating','eaten','eats','ran','running','runs','runner','ate','bats','batted','battering','dance','danced','dancing']\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for w in words:\n",
    "    print(f\"{w} --> {ps.stem(w)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db0a78f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat --> eat\n",
      "eating --> eat\n",
      "eaten --> eaten\n",
      "eats --> eat\n",
      "ran --> ran\n",
      "running --> runn\n",
      "runs --> run\n",
      "runner --> runn\n",
      "ate --> ate\n",
      "bats --> bat\n",
      "batted --> batt\n",
      "battering --> batter\n",
      "dance --> dance\n",
      "danced --> danc\n",
      "dancing --> danc\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "rs = RegexpStemmer('ing$|ed$|s$|able$|er$')\n",
    "\n",
    "for w in words:\n",
    "    print(f\"{w} --> {rs.stem(w)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13487780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat --> eat\n",
      "eating --> eat\n",
      "eaten --> eaten\n",
      "eats --> eat\n",
      "ran --> ran\n",
      "running --> run\n",
      "runs --> run\n",
      "runner --> runner\n",
      "ate --> ate\n",
      "bats --> bat\n",
      "batted --> bat\n",
      "battering --> batter\n",
      "dance --> danc\n",
      "danced --> danc\n",
      "dancing --> danc\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "ss = SnowballStemmer('english')\n",
    "\n",
    "for w in words:\n",
    "    print(f\"{w} --> {ss.stem(w)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45831ecd",
   "metadata": {},
   "source": [
    "### Lemmatization ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53727545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat --> eat\n",
      "eating --> eat\n",
      "eaten --> eat\n",
      "eats --> eat\n",
      "ran --> run\n",
      "running --> run\n",
      "runs --> run\n",
      "runner --> runner\n",
      "ate --> eat\n",
      "bats --> bat\n",
      "batted --> bat\n",
      "battering --> batter\n",
      "dance --> dance\n",
      "danced --> dance\n",
      "dancing --> dance\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "for w in words:\n",
    "    print(f\"{w} --> {wnl.lemmatize(w, pos='v')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e2ab1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat --> eat\n",
      "eating --> eating\n",
      "eaten --> eaten\n",
      "eats --> eats\n",
      "ran --> ran\n",
      "running --> running\n",
      "runs --> run\n",
      "runner --> runner\n",
      "ate --> ate\n",
      "bats --> bat\n",
      "batted --> batted\n",
      "battering --> battering\n",
      "dance --> dance\n",
      "danced --> danced\n",
      "dancing --> dancing\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "for w in words:\n",
    "    print(f\"{w} --> {wnl.lemmatize(w, pos='n')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "608ab5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat --> eat\n",
      "eating --> eating\n",
      "eaten --> eaten\n",
      "eats --> eats\n",
      "ran --> ran\n",
      "running --> running\n",
      "runs --> runs\n",
      "runner --> runner\n",
      "ate --> ate\n",
      "bats --> bats\n",
      "batted --> batted\n",
      "battering --> battering\n",
      "dance --> dance\n",
      "danced --> danced\n",
      "dancing --> dancing\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "for w in words:\n",
    "    print(f\"{w} --> {wnl.lemmatize(w, pos='a')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cc1b28",
   "metadata": {},
   "source": [
    "### Text Preprocessing - Filtering Stop words , Stemming and Lemmatization ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5705a1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "\n",
    "# stopwords.words('english')\n",
    "sentences= nltk.tokenize.sent_tokenize(corpus)\n",
    "\n",
    "#Apply Stopwords Removal and Stemming\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words =nltk.word_tokenize(sentences[i])\n",
    "    words=[stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words) # Convert list of words back to sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "69841f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snowball Stemming and Removal of Stopwords: ['to generat paragraph natur , use one mani ai-pow paragraph generat tool avail onlin .', 'these tool use artifici intellig natur languag process creat origin content base specif , topic , tone , length .']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Snowball Stemming and Removal of Stopwords:\",sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d27fc690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# stopwords.words('english')\n",
    "sentences= nltk.tokenize.sent_tokenize(corpus)\n",
    "\n",
    "#Apply Stopwords Removal and Stemming\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words =nltk.word_tokenize(sentences[i])\n",
    "    words=[lemmatizer.lemmatize(word.lower(),pos='v') for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words) # Convert list of words back to sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "250d0e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordNet Lemmatizing and Removal of Stopwords: ['to generate paragraph nature , use one many ai-powered paragraph generator tool available online .', 'these tool use artificial intelligence natural language process create original content base specifications , topic , tone , length .', 'the transformer architecture deep learn model use self-attention mechanism process sequential data , like text , without rely recurrence .', \"this architecture , introduce 2017 paper `` attention be all you need '' , structure around encoder decoder , efficiently process input sequence generate output sequence capture relationships elements regardless distance .\"]\n"
     ]
    }
   ],
   "source": [
    "print(f\"WordNet Lemmatizing and Removal of Stopwords:\",sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1808aaba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
